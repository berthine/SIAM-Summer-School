{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIAM2021 - Linear Reg 2 (long).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/SIAM-Summer-School/blob/main/SIAM2021_Linear_Reg_2_(long).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eK-854jPC00"
      },
      "source": [
        "## Practical: Introduction (2) to linear regression using Tensorflow (long approach)\n",
        "(19/July/2021)\n",
        "\n",
        "### 2021 Gene Golub SIAM Summer School \n",
        "https://sites.google.com/aims.ac.za/g2s3/home \n",
        "\n",
        "Instructor\n",
        "\n",
        "<font color=\"green\">***Dr. Emmanuel Dufourq*** \n",
        "\n",
        "www.emmanueldufourq.com\n",
        "\n",
        "edufourq (['@']) gmail.com\n",
        "\n",
        "***African Institute for Mathematical Sciences***\n",
        "\n",
        "***Stellenbosch University***\n",
        "\n",
        "***2021***\n",
        "\n",
        "\n",
        "Material adapted from:\n",
        "\n",
        "https://d2l.ai/chapter_linear-networks/linear-regression-scratch.html\n",
        "\n",
        "https://www.tensorflow.org/guide/basic_training_loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85NCvVuJko9u"
      },
      "source": [
        "## <font color=\"green\"> Learning outcomes:\n",
        "\n",
        "* Implement your own model using two features. Check your understanding of the previous notebook\n",
        "\n",
        "\n",
        "## <font color=\"green\">Data information:\n",
        "\n",
        "* Features: two real valued features\n",
        "\n",
        "* Output: one real valued label\n",
        "\n",
        "## <font color=\"green\">Tasks for participants (boolean)?\n",
        "\n",
        "* Yes, at the end (try avoid copy/pasting code, rather write it out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cVRaZ4YPa7i"
      },
      "source": [
        "In this notebook we'll create a dataset that has two features. We will create a linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lrD1PE5PKns"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXPN3KU6QIQK"
      },
      "source": [
        "## First we generate some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z6mdDK1QFSz"
      },
      "source": [
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "    X = tf.zeros((num_examples, w.shape[0]))\n",
        "    X += tf.random.normal(shape=X.shape)\n",
        "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n",
        "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
        "    y = tf.reshape(y, (-1, 1))\n",
        "    return X, y\n",
        "\n",
        "# True values\n",
        "true_w = tf.constant([4, -3.4])\n",
        "true_b = 4.2\n",
        "\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvWLWBdyW5d"
      },
      "source": [
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JKYYnNNyW7q"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgyRBrySy6bR"
      },
      "source": [
        "## Plot the data\n",
        "\n",
        "Plotting the first feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_pvGdH_P20X"
      },
      "source": [
        "plt.scatter(features[:,0].numpy(), labels.numpy(), c=\"b\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4z_QKwIy_sa"
      },
      "source": [
        "Plotting the second feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hM13MEVynmn"
      },
      "source": [
        "plt.scatter(features[:,1].numpy(), labels.numpy(), c=\"b\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgNJOkkzRhN8"
      },
      "source": [
        "Recall that training models consists of making multiple passes over the dataset, grabbing one minibatch of examples at a time, and using them to update our model. \n",
        "\n",
        "Each minibatch consists of a tuple of features and labels.\n",
        "\n",
        "The function below creates an iterator which can generate mini-batches of X-y pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3OHbtWXP7_j"
      },
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    \n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        j = tf.constant(indices[i:min(i + batch_size, num_examples)])\n",
        "\n",
        "        # Return a tuple of features and labels\n",
        "        yield tf.gather(features, j), tf.gather(labels, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xt2tgL2SmYJ"
      },
      "source": [
        "Let's take a look at a mini batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43DIqFVFQ7Xh"
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print ('mini batch X')\n",
        "    print(X)\n",
        "    print ('\\nmini batch Y')\n",
        "    print (y)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PozOzgeiUns5"
      },
      "source": [
        "## To do: Define the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukj8UMcFUDno"
      },
      "source": [
        "class MySecondModel(tf.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    # Let's re-define our variables here\n",
        "    self.w = # to do\n",
        "    self.b = # to do\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return # to do"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvjR4EO7Vhys"
      },
      "source": [
        "linear_reg = # to do"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P9Iapwk3ZfC"
      },
      "source": [
        "## To do: display the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLg5dj32Vj_w"
      },
      "source": [
        "# to do"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pi6JG2HYqWT"
      },
      "source": [
        "Plot the predictions before the optimisation process\n",
        "\n",
        "Feature 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNBfgAhm04Xi"
      },
      "source": [
        "plt.scatter(X[:,0], y, c=\"b\")\n",
        "plt.scatter(X[:,0], linear_reg(X).numpy(), c=\"r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNzQfPsJ07uB"
      },
      "source": [
        "Feature 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5iHMvD0XZJh"
      },
      "source": [
        "plt.scatter(X[:,1], y, c=\"b\")\n",
        "plt.scatter(X[:,1], linear_reg(X).numpy(), c=\"r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Q5vkXWKHg"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "reduce_mean : https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean?hl=en\n",
        "\n",
        "squre : https://www.tensorflow.org/api_docs/python/tf/math/square?hl=en"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GUiWojUV5En"
      },
      "source": [
        "# This computes a single loss value for an entire batch\n",
        "def loss(target_y, predicted_y):\n",
        "  return tf.reduce_mean(tf.math.square(target_y - predicted_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7OKJjIXYxb6"
      },
      "source": [
        "## TO do: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdVK5GF2Yxic"
      },
      "source": [
        "# Define a learning rate\n",
        "lr = # to do\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = # to do\n",
        "\n",
        "# We will keep track of the weights so we can plot them over the epochs\n",
        "Ws_0, bs, Ws_1= [], [], []\n",
        "\n",
        "# Iterate for a number of epochs (1)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # In each epoch generate batches of training data (2)\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "\n",
        "        # Trainable variables are automatically tracked by GradientTape (3)\n",
        "        with tf.GradientTape() as g:\n",
        "            l = # to do\n",
        "\n",
        "        # Compute gradient on l with respect to [`w`, `b`] which are on\n",
        "        # inside the model (self.w and self.b) (4)\n",
        "        dw, db = # to do\n",
        "\n",
        "        # Subtract the gradient scaled by the learning rate (5)\n",
        "        linear_reg.w.assign_sub(# to do)\n",
        "        linear_reg.b.assign_sub(# to do)\n",
        "\n",
        "    # Keep track of the weights so we can make a nice plot\n",
        "    Ws_0.append(linear_reg.w.numpy()[0])\n",
        "    Ws_1.append(linear_reg.w.numpy()[1])\n",
        "    bs.append(linear_reg.b.numpy())\n",
        "\n",
        "    # Compute this epochs's training loss\n",
        "    train_l = loss(linear_reg(X), y)\n",
        "\n",
        "    # Print to the screen\n",
        "    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}, w: {linear_reg.w.numpy()}, b: {linear_reg.b.numpy()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YVEjUoF4eKX"
      },
      "source": [
        "## To do:\n",
        "\n",
        "spend some time famialising yourself with steps (1) to (5) above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSB2mjNhFVj"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Plot the change in weights compared to the true ones\n",
        "\n",
        "First feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqpwK3nQY2s2"
      },
      "source": [
        "plt.plot(range(num_epochs), Ws_0, \"r\")\n",
        "\n",
        "plt.plot([true_w.numpy()[0]]  * len(range(num_epochs)), \"b--\")\n",
        "\n",
        "plt.legend([\"W\", \"True W\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtpKZ1rn2oXd"
      },
      "source": [
        "Second feature and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK05a11P1Ntw"
      },
      "source": [
        "plt.plot(range(num_epochs), Ws_1, \"r\",\n",
        "         range(num_epochs), bs, \"b\")\n",
        "\n",
        "plt.plot([true_w.numpy()[1]] * len(range(num_epochs)), \"r--\",\n",
        "         [true_b] * len(range(num_epochs)), \"b--\")\n",
        "\n",
        "plt.legend([\"W\", \"b\", \"True W\", \"True b\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zav6zX3q11c7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}