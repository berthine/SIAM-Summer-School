{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretrained Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/SIAM-Summer-School/blob/main/Pretrained_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAgr8ur7Fs4s"
      },
      "source": [
        "## Practical: Pretraining networks - transfer learning\n",
        "(20/July/2021)\n",
        "\n",
        "### 2021 Gene Golub SIAM Summer School \n",
        "https://sites.google.com/aims.ac.za/g2s3/home \n",
        "\n",
        "Instructor\n",
        "\n",
        "<font color=\"green\">***Dr. Emmanuel Dufourq*** \n",
        "\n",
        "www.emmanueldufourq.com\n",
        "\n",
        "edufourq (['@']) gmail.com\n",
        "\n",
        "***African Institute for Mathematical Sciences***\n",
        "\n",
        "***Stellenbosch University***\n",
        "\n",
        "***2021***\n",
        "\n",
        "\n",
        "material adapted from: https://neptune.ai/blog/transfer-learning-guide-examples-for-images-and-text-in-keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmhVUtUKFOqA"
      },
      "source": [
        "![](https://storage.googleapis.com/kaggle-competitions/kaggle/3362/media/woof_meow.jpg)\n",
        "\n",
        "***Deep Blue beat Kasparov at chess in 1997.***\n",
        "\n",
        "***Watson beat the brightest trivia minds at Jeopardy in 2011.***\n",
        "\n",
        "***Google DeepMind's AlphaGo outperformed European Go champion Fan Hui in 2015.***\n",
        "\n",
        "***Can your neural network tell Fido from Mittens in 2021?***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flnO_4mSh6-Z"
      },
      "source": [
        "## <font color=\"green\"> Learning outcomes:\n",
        "\n",
        "* How to use ```image_dataset_from_directory``` to read data from a folder\n",
        "\n",
        "* The ```GlobalAveragePooling2D``` layer\n",
        "\n",
        "* How to implement a pre-trained network, in this case MobileNetV2\n",
        "\n",
        "* How to implement ```preprocess_input``` from one of the ```tensorflow.keras.applications```\n",
        "\n",
        "* Downloading data from somewhere and using it in Google Colab\n",
        "\n",
        "## <font color=\"green\">Data information:\n",
        "\n",
        "* Features: (150x150x3) images\n",
        "\n",
        "* Output: 2 classes\n",
        "\n",
        "## <font color=\"green\">Tasks for participants (boolean)?\n",
        "\n",
        "* No, follow along and make your own modifications and make sure you understand\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-StJmPx79YgU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e46g7GyBF79V"
      },
      "source": [
        "## First, let's download the cats vs dogs dataset.\n",
        "\n",
        "Dataset https://www.kaggle.com/c/dogs-vs-cats \n",
        "\n",
        "\"In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-GJkM2t-nHa",
        "outputId": "03aff191-acf8-4a5a-b411-a4348ade5305"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://namespace.co.ke/ml/dataset.zip \\\n",
        "    -O /content/catsdogs.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-14 10:31:06--  https://namespace.co.ke/ml/dataset.zip\n",
            "Resolving namespace.co.ke (namespace.co.ke)... 109.106.250.14\n",
            "Connecting to namespace.co.ke (namespace.co.ke)|109.106.250.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228894139 (218M) [application/zip]\n",
            "Saving to: ‘/content/catsdogs.zip’\n",
            "\n",
            "/content/catsdogs.z 100%[===================>] 218.29M  2.87MB/s    in 81s     \n",
            "\n",
            "2021-07-14 10:32:28 (2.71 MB/s) - ‘/content/catsdogs.zip’ saved [228894139/228894139]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKhf1cM3GWSM"
      },
      "source": [
        "Now we extract the data into two folders. Check your \"file\" on the left. You should see the .zip file and a new folder cats_dogs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5easQd9q-nJ9"
      },
      "source": [
        "with zipfile.ZipFile('catsdogs.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/cats_dogs')\n",
        "base_dir = '/content/cats_dogs/dataset'\n",
        "train_dir = os.path.join(base_dir, 'training_set')\n",
        "validation_dir = os.path.join(base_dir, 'test_set')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppfYq3wPBGAs"
      },
      "source": [
        "Tensorflow has a class called image_dataset_from_directory that allows you to read in images from a particular folder and create a dataset. There is a particular argument called ```label_mode='categorical'``` which helps us create one hot encoded targets! API: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\n",
        "\n",
        "Since the training directory has two subfolders 'cats' and 'dogs' those will be the two classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-2-Q671Gu1-"
      },
      "source": [
        "Let's create the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbGb9L6-nMT",
        "outputId": "d36d1b25-44fe-4277-be81-920a0e466347"
      },
      "source": [
        "training_set = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=32,\n",
        "                                             label_mode='categorical',\n",
        "                                             image_size=(150, 150))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF_fD6W4Gwug"
      },
      "source": [
        "Now the testing data.\n",
        "\n",
        "Since the testing directory has two subfolders 'cats' and 'dogs' those will be the two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4we_J9_-nRQ",
        "outputId": "41464f11-6544-41fc-ba92-639c1dba72c4"
      },
      "source": [
        "test_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=32,\n",
        "                                                  label_mode='categorical',\n",
        "                                                  image_size=(150, 150))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9QMcfvGzSR"
      },
      "source": [
        "## Download a pre-trained model.\n",
        "\n",
        "Here we download MobileNetV2, API here: https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2\n",
        "\n",
        "The ```include_top``` is an important arguement here, formally, \"Boolean, whether to include the fully-connected layer at the top of the network.\" Since we want to add our own output softmax layer, we set the variable to False. When setting it to False, we essentiall get the 'feature extractor' and no 'classifier' part of the network. MobileNetV2 was pre-trained on a lot of classes (1000) and our problem only has two classes so we don't need the original softmax layer nor any other fully connected layers in the 'classifier' part.\n",
        "\n",
        "We also tell it to load the 'imagenet' weights, i.e. the 1000 class problem it was originally trained on. This means the model won't be initialised with random weights, but with the weights obtained from pre-training on ImageNet.\n",
        "\n",
        "There are a number of models that we can download, API here: https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
        "\n",
        "The approach we take in building this model is similar to the Functional API we examined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZ-vR-O-ylN",
        "outputId": "6e664b10-8d4b-4a43-af99-a1126dbb3ace"
      },
      "source": [
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',  \n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCp_sGcA_C5q"
      },
      "source": [
        "Since many pre-trained models have a `tf.keras.layers.BatchNormalization` layer, it’s important to freeze those layers. Otherwise, the layer mean and variance will be updated, which will destroy what the model has already learned.\n",
        "\n",
        "Let’s freeze all the layers in this case. You'll notice when we eventually print the model.summary() there will be a bunch of non-trainable weights. In other words, when setting .trainable to False, we now ensure that the weights loaded do not update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vzw6FUq-yn2"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pICfgW-M8fSv"
      },
      "source": [
        "Let's create a new input for our dataset. Our dataset has colour images of shape 150 x 150."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06GfyDp0-yqq"
      },
      "source": [
        "inputs = Input(shape=(150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2DdjO3i8kdz"
      },
      "source": [
        "Note: each Keras Application expects a specific kind of input preprocessing. For MobileNetV2, call ```tf.keras.applications.mobilenet_v2.preprocess_input``` on your inputs before passing them to the model. For another model, you will have to use a different ```process_input```!\n",
        "\n",
        "```mobilenet_v2.preprocess_input``` will scale input pixels between -1 and 1. API for mobilenetv2 preprocess: \n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/preprocess_input\n",
        "\n",
        "Normally, we would scale the values between 0 and 1 by dividing by 255, but in this case we don't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJxlilOO-yvX"
      },
      "source": [
        "x = preprocess_input(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12No7deV_VfH"
      },
      "source": [
        "ensure that the base model is running in inference mode so that batch normalization layers are not updated during the fine-tuning stage (set `training=False`). Note that we are freezing the feature extractor part of the pre-trained network. We will add on new layers and those layers will be updated via normal backpropagation.\n",
        "\n",
        "Between we take the output from the last pooling layer in the pre-trained network and connect it to a new ```GlobalAveragePooling2D``` layer. Followed by dropout and a new fully connected layer. All of these will have new randomly initialised weights which will be updated.\n",
        "\n",
        "Finally, we create the model which has 1 input branch and 1 output branch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPmujgIn-yxu"
      },
      "source": [
        "x = base_model(x, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)  \n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZu5Psok_Nbg",
        "outputId": "7f69ff26-f215-460f-fb7d-50c28e3e916b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "tf.math.truediv (TFOpLambda) (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "tf.math.subtract (TFOpLambda (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "mobilenetv2_1.00_224 (Functi (None, 5, 5, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 2562      \n",
            "=================================================================\n",
            "Total params: 2,260,546\n",
            "Trainable params: 2,562\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHAd-eHN9uMM"
      },
      "source": [
        "Compile as normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6A62lzt_NeC"
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAFLGlZK9vgF"
      },
      "source": [
        "Train as normal, except now our training features and labels is a tf.data.Dataset object because of the ```image_dataset_from_directory``` that we used earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ydoZrs_Ngs",
        "outputId": "ca1628b9-2054-4eb5-a3e8-268c3b5d650b"
      },
      "source": [
        "model.fit(training_set, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "250/250 [==============================] - 54s 94ms/step - loss: 0.1651 - accuracy: 0.9371\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.0894 - accuracy: 0.9691\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 23s 93ms/step - loss: 0.0776 - accuracy: 0.9734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe0419fab10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcz4gCe9-82"
      },
      "source": [
        "Let's create an array to store the predictions and true labels for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KrDE3oBsyq"
      },
      "source": [
        "predictions = np.array([])\n",
        "labels =  np.array([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrvfU4RJ-DHF"
      },
      "source": [
        "Now let's iterate on the test data (a tf.data.Dataset object) and predict along with obtaining the true values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nkMYesuEsmD"
      },
      "source": [
        "for x, y in test_dataset:\n",
        "  predictions = np.concatenate([predictions, np.argmax(model.predict(x),axis=-1)])\n",
        "  labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIIGNwZJCO8D",
        "outputId": "9d3d8abd-a0a7-4558-b008-70c66f835b49"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., ..., 1., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDXMuftJCQdj",
        "outputId": "83458f60-3b19-45ac-8dbf-af0421224e47"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., ..., 1., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYycNZWh-L84"
      },
      "source": [
        "Finally, we end with the confusion matrix and the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL0dAfsACc3w",
        "outputId": "4ff0d7f5-311d-4106-b5c0-ec2449a2b8bd"
      },
      "source": [
        "confusion_matrix(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[971,  29],\n",
              "       [ 25, 975]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR6FLsgXCizB",
        "outputId": "094234f6-494c-4b05-ebb6-0dee68f25a2c"
      },
      "source": [
        "accuracy_score(labels,predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A4BC11FEKzR"
      },
      "source": [
        "## Predict on images downloaded from Google Images.\n",
        "\n",
        "Download some images of cats and some of dogs from the Internet and upload them for prediction.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory note that the encoded are obtained by sorting the sub folders in order, so cat would be first and dog second, so cat would be encoded as [1 0] and dog as [0 1], if you had to use np.argmax() then cat would be the integer 0 and dog the integer 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6H8sZuNCuC8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "c35c74c4-239a-4674-c4e6-78f39b8b4aae"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print('file:',fn)\n",
        "  print('softmax output',classes)\n",
        "  print ('integer output',np.argmax(classes,axis=-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab89352f-e47a-448e-900f-dd3e5dc88c8d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab89352f-e47a-448e-900f-dd3e5dc88c8d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-i1sZfg-S6u"
      },
      "source": [
        "## Task\n",
        "\n",
        "* Re-run the code but this time **do not train the model**. Instead, predict directly on the test data and compare the performance. The model should do badly because the new fully connected layer was initialised with random values and not trained.\n",
        "\n",
        "* Try to modify the classifier part of the network. In this example we used  ```GlobalAveragePooling2D``` following by ```Dropout``` but perhaps you could add another fully connected layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP_Q1EL4Dysh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}